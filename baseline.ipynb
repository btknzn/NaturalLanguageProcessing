{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import meta info (tokens, number of users )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = json.load(open('./meta.json', 'r'))\n",
    "tokens = meta['tokens']\n",
    "num_token = len(tokens)\n",
    "num_user = meta['num_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset, there are 13369 number of tokens (words) and these tweets are from 8 users\n"
     ]
    }
   ],
   "source": [
    "print('In dataset, there are {} number of tokens (words) and these tweets are from {} users'.format(num_token, num_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and validataion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('./train.json', 'r'))\n",
    "valid_data = json.load(open('./valid.json', 'r'))\n",
    "s_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 tweets in train dataset, 356 tweets in valid dataset.\n",
      "Each json file is a list of dictionary, and each dictionary has information of tweets\n",
      "[TWEET INFO]: user id, sentence, processed token id.\n",
      "\n",
      "Sample train data:  {'user_id': 0, 'sentence': 'i recently met lakeisha crum the first in her family to go to college loved her story', 'token_id': [5721, 9659, 7459, 6629, 2686, 11853, 4447, 5870, 5460, 4236, 12017, 4981, 12017, 2197, 7047, 5460, 11310]}\n",
      "\n",
      "Note that: tokens.index(word) = token_id\n",
      "\n",
      "Example:\n",
      "[5721, 9659, 7459, 6629, 2686, 11853, 4447, 5870, 5460, 4236, 12017, 4981, 12017, 2197, 7047, 5460, 11310]\n",
      "[5721, 9659, 7459, 6629, 2686, 11853, 4447, 5870, 5460, 4236, 12017, 4981, 12017, 2197, 7047, 5460, 11310]\n"
     ]
    }
   ],
   "source": [
    "print('{} tweets in train dataset, {} tweets in valid dataset.'.format(len(train_data), len(valid_data)))\n",
    "print('Each json file is a list of dictionary, and each dictionary has information of tweets')\n",
    "print('[TWEET INFO]: user id, sentence, processed token id.')\n",
    "print()\n",
    "\n",
    "print('Sample train data: ', train_data[s_idx])\n",
    "print()\n",
    "print('Note that: tokens.index(word) = token_id')\n",
    "print()\n",
    "print('Example:')\n",
    "print(train_data[0]['token_id'])\n",
    "print([tokens.index(w) for w in train_data[s_idx]['sentence'].split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset and DataLoader\n",
    "- Note that below code for dataset and dataloader only supports `batch_size = 1`.\n",
    "- Try to find out a way to batchfy the data.\n",
    "- Even if you batchfy the data, put the `token_id` information into `sample['token_id']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample['token_id'] = torch.Tensor(sample['token_id'])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tweetDataset(train_data)\n",
    "valid_dataset = tweetDataset(valid_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample datapoint information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from train dataloader: \n",
      "USER ID:  tensor([5])\n",
      "TOKEN ID:  tensor([[13111., 11092., 11990., 11853.,  4408.,  6661., 13196.,  1048., 12924.,\n",
      "         11842.,  2258.,  3885.,  7569.,  7116.,  5870.,  7319.,  6993.]])\n",
      "TOKEN ID shape should be BATCH by LENGTH:  torch.Size([1, 17])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_dataloader))\n",
    "\n",
    "print('Sample from train dataloader: ')\n",
    "print('USER ID: ', sample['user_id'])\n",
    "print('TOKEN ID: ', sample['token_id'])\n",
    "print('TOKEN ID shape should be BATCH by LENGTH: ', sample['token_id'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model based on LSTM\n",
    "- Note that below code for model only supports `batch_size = 1`.\n",
    "- Try to find out a way to use mini-batch.\n",
    "\n",
    "```diff\n",
    "- You must make your class name as \"Model\", as below.\n",
    "- You must make your model work with the input of sample['token_id']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_token, num_user, embed_dim, rnn_dim, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_token = num_token\n",
    "        self.num_user = num_user\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_token, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.out_linear = nn.Linear(rnn_dim, num_user)\n",
    "        \n",
    "    def forward(self, token_id):\n",
    "        embed = self.embedding(token_id)\n",
    "        out, _ = self.rnn(embed)\n",
    "        return self.out_linear(out[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an instance of model and define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model = Model(num_token, num_user, embed_dim=512, rnn_dim=1024, num_layers=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of parameter information\n",
    "```diff\n",
    "- The number of parameters should not exceed 20,000,000 !!\n",
    "- DO NOT USE TRANSFORMER-BASED MODELS!!\n",
    "- Transformer-based models will not be accepted as a submission.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 13152776\n",
      "[NOTE] Number of parameters SHOULD NOT exceed 20,000,000 (20 million).\n"
     ]
    }
   ],
   "source": [
    "num_param = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters: {}'.format(num_param))\n",
    "print('[NOTE] Number of parameters SHOULD NOT exceed 20,000,000 (20 million).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "```diff\n",
    "- Test the model if it generates proper output, which shape is B by num_user\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape would be BATCH X NUM_USER/OUTPUT :  torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "pred = model(sample['token_id'].long().to(device))\n",
    "print('Prediction shape would be BATCH X NUM_USER(OUTPUT) : ', pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825b0b9f560a49e696fe8cbb36ba535a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 0] BEST VALID ACCURACY UPDATED: 0.3904494345188141\n",
      "[EPOCH 1] BEST VALID ACCURACY UPDATED: 0.45505619049072266\n",
      "[EPOCH 2] BEST VALID ACCURACY UPDATED: 0.5758426785469055\n",
      "[EPOCH 3] BEST VALID ACCURACY UPDATED: 0.617977499961853\n",
      "[EPOCH 5] BEST VALID ACCURACY UPDATED: 0.6348314881324768\n",
      "[EPOCH 6] BEST VALID ACCURACY UPDATED: 0.6404494643211365\n",
      "[EPOCH 7] BEST VALID ACCURACY UPDATED: 0.6404494643211365\n",
      "[EPOCH 8] BEST VALID ACCURACY UPDATED: 0.6432584524154663\n",
      "[EPOCH 9] BEST VALID ACCURACY UPDATED: 0.6432584524154663\n",
      "[EPOCH 10] BEST VALID ACCURACY UPDATED: 0.6432584524154663\n",
      "[EPOCH 13] BEST VALID ACCURACY UPDATED: 0.648876428604126\n",
      "[EPOCH 14] BEST VALID ACCURACY UPDATED: 0.6516854166984558\n",
      "[EPOCH 15] BEST VALID ACCURACY UPDATED: 0.6516854166984558\n",
      "[EPOCH 17] BEST VALID ACCURACY UPDATED: 0.6713483333587646\n",
      "[EPOCH 18] BEST VALID ACCURACY UPDATED: 0.6797752976417542\n",
      "[EPOCH 27] BEST VALID ACCURACY UPDATED: 0.6910112500190735\n"
     ]
    }
   ],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "avg_loss = 0.0\n",
    "best_valid_accu = 0.0\n",
    "best_epoch = -1\n",
    "best_model = None\n",
    "num_epoch = 30\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    # start training\n",
    "    for sample in train_dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(sample['token_id'].long().to(device))\n",
    "\n",
    "        loss = criteria(pred, sample['user_id'].long().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item() / len(train_dataloader)\n",
    "\n",
    "    # start validation\n",
    "    correct_cnt = 0.0\n",
    "    data_cnt = 0.0\n",
    "    for sample in valid_dataloader:\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(sample['token_id'].long().to(device))\n",
    "\n",
    "        pred_user_id = torch.argmax(pred, dim=-1)\n",
    "\n",
    "        accu = pred_user_id.detach().cpu() == sample['user_id']\n",
    "\n",
    "        correct_cnt += torch.sum(accu)\n",
    "        data_cnt += sample['token_id'].shape[0]\n",
    "\n",
    "    # calculate best valid accuracy, and save the best model. \n",
    "    curr_valid_accu = (correct_cnt / data_cnt).item()\n",
    "\n",
    "    best_valid_accu = max(best_valid_accu, curr_valid_accu)\n",
    "    if best_valid_accu == curr_valid_accu:\n",
    "        best_epoch = epoch\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), 'best_baseline.pth')\n",
    "        print('[EPOCH {}] BEST VALID ACCURACY UPDATED: {}'.format(epoch, best_valid_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED TRAINING : BEST MODEL AT EPOCH 27 WITH ACCURACY 0.6910112500190735\n"
     ]
    }
   ],
   "source": [
    "print('FINISHED TRAINING : BEST MODEL AT EPOCH {} WITH ACCURACY {}'.format(best_epoch, best_valid_accu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
